# Hive
    * å¤„ç†çš„æ•°æ®å­˜å‚¨åœ¨Hadoopä¸­
    * åˆ†æåº•å±‚å®ç°äº†MapReduce
    * æ‰§è¡Œè¿è¡Œåœ¨yarnä¸­
    
    æ˜¯ä»€ä¹ˆï¼š
        * FaceBookå¼€æºçš„è§£å†³æµ·é‡ç»“æ„åŒ–æ—¥å¿—æ•°æ®ç»Ÿè®¡
        * Hiveræ˜¯åŸºäºHadoopçš„æ•°æ®ä»“åº“å·¥å…·ï¼Œå°†ç»“æ„åŒ–æ•°æ®æ–‡ä»¶æ˜ å°„æˆä¸€å¼ è¡¨
        
        * å»ºç«‹åœ¨Hadoopä¹‹ä¸Šä½¿ç”¨HQLæŸ¥è¯¢æ¥å£
        * ä½¿ç”¨HDFSå­˜å‚¨æ•°æ®
        * ä½¿ç”¨MapReduceè®¡ç®—ã€‚æœ¬è´¨æ˜¯å°†HQLè½¬æ¢æˆMapReducuç¨‹åº
        * çµæ´»æ€§æ‰©å±•æ€§æ¯”è¾ƒå¥½ï¼šæ”¯æŒUDF
        
        * é€‚åˆç¦»çº¿æ•°æ®åˆ†æ
    
## RDBMS
    è¡¨çš„æ¦‚å¿µï¼šä¸åˆ—å¯¹åº”èµ·æ¥
        create table{
            
        }
        
## Hive å®‰è£…
    * ä¿®æ”¹hive-env.shé…ç½®æ–‡ä»¶(ä¹Ÿå¯ä»¥åœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®)
        export HADOOP_HOME
        export HIVE_HOME
    
    * åˆå§‹åŒ–defaultæ•°æ®åº“
        schematool -dbType derby -initSchema
        
        ps:é»˜è®¤åœ¨å†…å­˜æ•°æ®åº“ä¸­å­˜å‚¨matastore
    * hive è¿›å…¥æ§åˆ¶å°
    * show databases;
    * use default;
    * show tables;
    * set; æŸ¥çœ‹å½“å‰æ‰€æœ‰é…ç½®
    
    bin/hdfs dfs -mkdir -p /user/hive/warehouse
    bin/hdfs dfs -chmod u+x /user/hive/warehouse
    
## ç®€å•å®ä¾‹
    * åˆ›å»ºè¡¨
        create table students(id string,name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
        delimited
        fields
    * student.txt
        10001   ydcun
        1002    joa
    * å¯¼å…¥æ•°æ®
        load data local inpath '/opt/app/hive-2.3.2/student.txt' into table students;
        
    * æŸ¥çœ‹è¡¨ä¿¡æ¯
        desc formatted tablename
        desc tablename
        desc function extended functionname
        desc function functionname
        
## Metastoreä¸‰ç§æ¨¡å¼
    * å†…åµŒDerbyæ–¹å¼
        schematool -dbType derby -initSchema
    * Localæ–¹å¼
        schematool -dbType mysql -initSchema
        <property>
                <name>hive.metastore.local</name>
                <value>local</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>root</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>123abc</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                        <value>jdbc:mysql://master:3306/matestore?createDatabaseIfNotExist=true</value>
        </property>
    * Remoteæ–¹å¼
    
  ## Hiveå®¢æˆ·ç«¯æ‰“å°å½“å‰æ•°æ®å’Œè¾“å‡ºè¡¨å¤´
        <property>
                <name>hive.cli.print.header</name>
                <value>true</value>
        </property> 
        <property>
                <name>hive.cli.print.current.db</name>
                <value>true</value>
        </property>
        
        å¯åŠ¨hiveå®¢æˆ·ç«¯ğŸˆ´ï¸æ—¶å€™è®¾ç½®å‚æ•°
            ä¾‹å¦‚ï¼šhive --hiveconf hive.root.logger=INFO,console
    
  ## metastore
        é»˜è®¤çš„defaultä¸ä¼šåœ¨warehouseé‡Œåˆ›å»ºæ–‡ä»¶å¤¹ï¼Œdefaultä¸­çš„è¡¨ç›´æ¥åœ¨warehouseä¸­åˆ›å»ºåŒåæ–‡ä»¶   
        
  ## Hiveäº¤äº’å‘½ä»¤
         -d,--define <key=value>          Variable substitution to apply to Hive
                                          commands. e.g. -d A=B or --define A=B
            --database <databasename>     Specify the database to use
         -e <quoted-query-string>         SQL from command line
         -f <filename>                    SQL from files
         -H,--help                        Print help information
            --hiveconf <property=value>   Use value for given property
            --hivevar <key=value>         Variable substitution to apply to Hive
                                          commands. e.g. --hivevar A=B
         -i <filename>                    Initialization SQL file
         -S,--silent                      Silent mode in interactive shell
         -v,--verbose                     Verbose mode (echo executed SQL to the
                                          console)
                                          
         hive -e 'select * from db_hive.students;'   -e -fä¸èƒ½åŒæ—¶ä½¿ç”¨
         hive -f 'sqlscript.sql'  
         hive -i 'init.sql'     ä¸ç”¨æˆ·è‡ªå®šä¹‰udfé…åˆä½¿ç”¨ -iå¯ä»¥ä¸ä»»æ„å‚æ•°ç»“åˆä½¿ç”¨       
         
         
## Hive å‘½ä»¤
    create table is not exists tablename(id int comment '', name string)
    
    create table tablename2 as select * from tablename;
       
    create table tablename like tablename;
    
    alter table tablename rename to name_rename
    
    * åˆ›å»ºå¤–éƒ¨è¡¨ï¼Œä¼šå°†æŒ‡å®šçš„æ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶éƒ½åŠ è½½åˆ°å¤–éƒ¨è¡¨ä¸­
        create EXTERNAL table student_ext(
        id string,
        name string
        )
        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
        LOCATION '/user/ydcun/student/'
        
    * åˆ†åŒºè¡¨ï¼ŒæŒ‡å®šçš„ç›®å½•ï¼Œä¸‹é¢çš„å­ç›®å½•å³ä¸ºåˆ†åŒº
        create EXTERNAL table student_part(
        id string,
        name string
        )
        PARTITIONED BY (month string)
        ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
        LOCATION '/user/ydcun/student_part/'
        
        load data local inpath '/opt/app/hive-2.3.2/student.txt' into table student_part partition(month='201712');
        
        select * from student_part where month='201712'; 
        
        ps:
            å¦‚æœç›´æ¥åœ¨dfsåˆ›å»ºæ–‡ä»¶å¤¹å¹¶åˆ›å»ºä»¥åˆ†åŒºå­—æ®µå‘½åçš„å­æ–‡ä»¶ï¼Œå› æ²¡æœ‰è®°å½•åˆ†åŒºä¿¡æ¯ä½¿å¾—è¡¨ä¸­æŸ¥ä¸åˆ°æ•°æ®
            æ–¹æ¡ˆ1ï¼šæ‰§è¡Œä¿®å¤ï¼š
                msck repair table tablename;
            æ–¹æ¡ˆ2ï¼š
                alter table tablename partition(month='201711')
                
            æŸ¥è¯¢åˆ†åŒºæ•°ï¼š
                show partitions tablename;
                
 ## æ–‡ä»¶å¯¼å‡º
    insert overwrite local directory '/user/ydcun/hive_exp_student'
    select * from students;
    insert overwrite local directory '/user/ydcun/hive_exp_student'
    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' COLLECTION ITEMS TERMINATED BY '\n'
    select * from students_ext;
    
    hive -e 'select * from student;' > /opt/app/hive-2.3.2/exp.txt
    
    psï¼š
        local å€’åˆ°æœ¬åœ°
        
 ## æ•°æ®å¤„ç†æµç¨‹
    * load          E
    * select python T
    * sub select    L        
    
## DML
    * æŸ¥çœ‹æ‰€æœ‰å‡½æ•°
        show functions;
    * æŸ¥çœ‹å‡½æ•°è¯´æ˜
        desc function max;
        
## exprot/import
    EXPORT TABLE student_ext TO '/user/ydcun/export/student_ext'; 
    IMPORT TABLE student_ext_imp FROM '/user/ydcun/export/student_ext'; 
    
## æ’åº
    order by å…¨å±€æ’åº
    
    sort by  reduceå†…æ’åº
        set mapreduce.job.reduces = 3;  //ç”Ÿæˆ3ä¸ª reduce è¾“å‡º
        select * from student_ext sort by id;    //ä¼šæœ‰ä¸‰ä¸ªreduceæ‰§è¡Œ
        
    distribute by   ç±»ä¼¼mapä¸­çš„åˆ†åŒºç»“åˆsort byè¿›è¡Œä½¿ç”¨
        select * from student_ext distribute by name sort by id;
        psï¼šdistibute by å¿…é¡»åœ¨sort byä¹‹å‰
        
    cluster by  sort by ä¸ distribute by çš„ç»„åˆï¼Œå½“è¿™ä¸¤ä¸ªæ’åºçš„å­—æ®µç›¸åŒçš„æ—¶å€™ç”¨clustger by       

## UDFï¼ˆuser definition functionï¼‰ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°
    show functions;  æŸ¥çœ‹è‡ªå¸¦å‡½æ•°
    desc extended function max;
    
    * UDF  ä¸€è¿›ä¸€å‡º
    * UDAF å¤šè¿›ä¸€å‡º  count
    * UDTF ä¸€è¿›å¤šå¤„  lateral 
    
    ä½¿ç”¨è¿‡ç¨‹ï¼š
        * ç¼–å†™å‡½æ•°
            <dependency>
              <groupId>org.apache.hive</groupId>
              <artifactId>hive-jdbc</artifactId>
              <version>${hiver.version}</version>
            </dependency>
            <dependency>
              <groupId>org.apache.hive</groupId>
              <artifactId>hive-exec</artifactId>
              <version>${hiver.version }</version>
            </dependency>
        * æ‰“æˆjaråŒ…
        * hiveä¸­  add jar '/****/**.jar'
        * create temporary function my_lower as "com.ydcun.hadoop.student.hive.udf.LowarFunction"
        * show function ä¸­å°±å°±å¯ä»¥çœ‹åˆ° my_lower è¿™ä¸ªå‡½æ•°
        
        æ–¹æ³•2ï¼š
            CREATE FUNCTION myfunc AS 'myclass' USING JAR 'hdfs:///path/to/jar';
            
            
 ## hiverservice2
    bin/hiveserver2
    ps -ef | grep java
    * beeline é“¾æ¥
        æ–¹æ³•1
            bin/beeline
            !connect jdbc:hive2://master:10000 root root
            
                psï¼šUser: root is not allowed to impersonate rootè¿™ä¸ªé”™è¯¯è§£å†³æ–¹æ³•
                    ä¿®æ”¹core-site.xmlæ–‡ä»¶ï¼š
                        rootç”¨æˆ·å¯ä»¥ä»£ç†å…¶å®ƒä»»æ„ç”¨æˆ·ï¼Œéœ€è¦é‡å¯hadoop
                        <property>
                          <name>hadoop.proxyuser.root.hosts</name>
                          <value>*</value>
                         </property>
                         <property>
                          <name>hadoop.proxyuser.root.groups</name>
                          <value>*</value>
                        </property>
                        
        æ–¹æ³•2 ç›´æ¥ç™»å½•
            bin/beeline -u jdbc:hive2://master:10000/default
        
        é€€å‡ºï¼š
            !quit
   
## å‹ç¼©
    * snappy 
        å…ˆå®‰è£…snappy   
        ç¼–è¯‘hadoopæºç ï¼Œæ·»åŠ snappyå¯é€‰é¡¹
            mvn package -Pdist,native,src -DskipTests -Dtar -Drequire.snappy
            
            ç¼–è¯‘çš„åº“åœ¨target/hadoop*/lib/native
        æ£€æŸ¥æ–¹æ³•ï¼š
        $ hadoop checknative
        17/12/27 13:16:47 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
        17/12/27 13:16:47 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
        Native library checking:
        hadoop:  true /opt/app/hadoop-2.7.5/lib/native/libhadoop.so.1.0.0
        zlib:    true /lib64/libz.so.1
        snappy:  false
        lz4:     true revision:99
        bzip2:   false
        openssl: false Cannot load libcrypto.so (libcrypto.so: æ— æ³•æ‰“å¼€å…±äº«å¯¹è±¡æ–‡ä»¶: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•)!

        æŸ¥çœ‹æ–‡ä»¶æˆ–ç›®å½•å¤§å°ï¼š
            hdfs dfs -du -h /user/ydcun/hive
            
            
## hiveä¼˜åŒ–
    * ä¸èµ°mapreduce
        hive.fetch.task.comversion     minimal|mores   
        minimalï¼šselect * | åˆ†åŒºè¡¨ | limit
        mores ï¼šminimal ï¼Œæ—¶é—´æˆ³ï¼Œè™šæ‹Ÿåˆ—
        
    * å¤§è¡¨æ‹†åˆ†
        create table   ã€‚ã€‚ã€‚ã€‚ as  select ã€‚ã€‚ã€‚ã€‚
    
    * å¤–éƒ¨è¡¨ï¼Œåˆ†åŒºè¡¨
        å¤šçº§åˆ†åŒº
    
    * æ•°æ®
        å­˜å‚¨ï¼štextfileï¼Œorcfileï¼Œparquet
        å‹ç¼©ï¼šsnappy
        
    * SQLè¯­å¥
        join
            * common| Shuffle| reduce join
                é“¾æ¥å‘ç”Ÿçš„é˜¶æ®µæ˜¯åœ¨reduceé˜¶æ®µï¼Œå¤§è¡¨å¯¹å¤§è¡¨çš„æ—¶å€™æ¯”è¾ƒé€‚åˆ
            * map join
                é“¾æ¥å‘ç”Ÿåœ¨mapé˜¶æ®µï¼Œå¤§è¡¨å¯¹å°è¡¨çš„æ—¶å€™æ¯”è¾ƒé€‚åˆ
                å¤§è¡¨æ•°æ®ä»ç¨³é‡è¯»å–ï¼Œ
                å°è¡¨æ•°æ®ä»å†…å­˜ä¸­åŠ è½½
                DistributedCacheè¿›è¡Œç¼“å­˜å°è¡¨å†…å®¹
                set hive.auto.convert.join=true;
            * SMB join  
                Sort-Merge-Bucket Join
                    joinçš„è¡¨é€šè¿‡ç›¸åŒçš„å­—æ®µè¿›è¡Œsortååˆ†åŒºï¼Œæ”¾åˆ°ä¸åŒçš„æ¡¶ä¸­ã€‚
                    é’ˆå¯¹ç›¸åŒçš„æ¡¶è¿›è¡Œå•ç‹¬joinå³å¯ï¼Œè§£å†³å¤§è¡¨å¯¹å¤§è¡¨çš„é—®é¢˜
                set hive.auto.convert.sortmerge.join=true;
                
                
    * hive æ‰§è¡Œè®¡åˆ’
    
        EXPLAIN [EXTENDED|DEPENDENCY|AUTHORIZATION] query
        
            explain select * from student_ext;
        
    * å¹¶è¡Œæ‰§è¡Œ
        hive.exec.parallel=ture;
        hive.exec.parallel.thread.number=8     //å»ºè®®ä¸è¦è¶…è¿‡20
        job1 a join b   aa
        job2 c join d   cc
        job3 aa join cc
        job1ä¸job2å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
        
    * JVMé‡ç”¨
        map task/reduce taskéƒ½éœ€è¦JVMç¯å¢ƒï¼ŒJVMé‡ç”¨èŠ‚çœJVMå¯ç”¨æ—¶é—´
        mapreduce.job.jvm.numtasks
     
    * Reduceæ•°ç›®
        é»˜è®¤æ˜¯å¿«çš„æ•°é‡å†³å®šçš„ã€‚
        mapreduce.job.reduces=1
        
    * æ¨æµ‹æ‰§è¡Œ
        hive.mapred.reduce.tasks.speculative.execution=true;
        mapreduce.map.speculative=true;
        mapreduce.reduce.speculative=true;
    * Mapæ•°ç›®
        hive.merge.size.per.task=256000000  å‚è€ƒå—çš„å¤§å°
        
    * ä¼ä¸šä¼˜åŒ–åŠ¨æ€åˆ†åŒº
        åˆ†åŒºï¼ˆmouthï¼Œdayï¼‰
        ydcun_log_ip
            /mouth=2017-11/day=25
        åŠ è½½æ•°æ®ï¼š
            load data path '' into table ydcun_log_ip partition(month='2017-1',day='01') 
        hive.exec.dynamic.partition=true;
        hive.exec.dynamic.partition.mode=strict    è‡³å°‘æœ‰ä¸€ä¸ªåˆ†åŒºæ˜¯é™æ€çš„
        hive.exec.dynamic.partition.pernode=100     æ¯ä¸ªmapperæˆ–reduceråˆ›å»ºçš„æœ€å¤§åŠ¨æ€åˆ†åŒºæ•°
        hive.exec.dynamic.partitions=100            ä¸€ä¸ªåŠ¨æ€åˆ†åŒºæ‰€åˆ›å»ºçš„æœ€å¤§åŠ¨æ€åˆ†åŒºæ•°
        hive.exec.max.created.files=10000           å…¨å±€å¯åˆ›å»ºçš„æœ€å¤§æ–‡ä»¶æ•°
        
        set hive.mapred.mode=nonstrict|strict;   
        strictæ˜¯ä¸¥æ ¼æ¨¡å¼ï¼šåˆ†åŒºè¡¨whereä¸­ä¸æŒ‡å®šåˆ†åŒºæŸ¥è¯¢ä¸åˆ°
                        å¯¹äºorder byå¿…é¡»ä½¿ç”¨limit
                        ç¬›å¡å°”ç§¯ä¸ä½¿ç”¨onè€Œä½¿ç”¨whereçš„ä¹Ÿå°†ç¦æ­¢
                        
                        
## å®æˆ˜
    åˆ›å»ºè¡¨ï¼šä½¿ç”¨æ­£åˆ™çš„æ–¹å¼
        CREATE TABLE serde_regex(
          host STRING,
          identity STRING,
          user STRING,
          time STRING,
          request STRING,
          status STRING,
          size STRING,
          referer STRING,
          agent STRING)
        ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
        WITH SERDEPROPERTIES (
          "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?",
          "output.format.string" = "%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s"
        )
        STORED AS TEXTFILE;
        
        